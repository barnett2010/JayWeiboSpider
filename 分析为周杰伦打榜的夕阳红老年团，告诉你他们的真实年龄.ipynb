{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析为周杰伦打榜的夕阳红老年团，告诉你他们的真实年龄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、需求背景\n",
    "\n",
    "## 二、功能描述\n",
    "\n",
    "## 三、技术方案\n",
    "我们大概分解为一下的技术步骤，以及使用的技术\n",
    "- 1、爬取`#周杰伦超话#`下的微博\n",
    "- 2、根据每条微博爬取该用户的基本信息\n",
    "- 3、将信息保存到csv文件\n",
    "- 4、使用数据分析用户的年龄、性别分布\n",
    "- 5、分析粉丝团的地区分布\n",
    "- 6、使用词云分析打榜微博的内容\n",
    "\n",
    "数据爬取使用的是`requests库`,保存csv文件我们可以使用内置的库`csv`,而可视化数据分析可以使用一个超级好用的pyecharts。\n",
    "\n",
    "## 四、爬取超话微博\n",
    "### 1、找到超话的加载的数据URL\n",
    "\n",
    "在谷歌浏览器中找到`#周杰伦超话#`页面，然后调出调试窗口，改为手机的模式，然后过滤请求，只查看异步请求，查看返回的数据格式，找到微博的内容所在！\n",
    "\n",
    "微博的请求连接为：\n",
    ">`https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type\n",
    "%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102`\n",
    "\n",
    "解码之后为：\n",
    ">https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed&extparam=周杰伦&luicode=10000011&lfid=100103type=61&q=周杰伦超话&t=0&sudaref=m.weibo.cn&display=0&retcode=6102\n",
    "\n",
    "可以截取前面的那一段：\n",
    ">https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed\n",
    "\n",
    "### 2、代码模拟请求数据\n",
    "\n",
    "拿到连接之后我们可以模拟请求，这里使用的是我们熟悉的`request库`,简单几句就可以获取到微博数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-7ca701234371>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7ca701234371>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    # spider_topic()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "\n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    \"\"\"\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "    print(r.text)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、提取微博的内容\n",
    "可以看到返回的内容是一个json格式的，一层层往下找，就可以找到微博的内容、用户id所在。\n",
    "\n",
    "了解微博的数据结构之后我们可以将微博的内容和id提取出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天也正常营业！！旁友们求互捞！！\n",
      "屯分，万一哪天又用上了呢？\n",
      "滴滴，第一次打卡ｸﾞｯ!(๑•̀ㅂ•́)و✧\n",
      "又是打榜的一天，周一加油~\n",
      "今日份，求捞！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "\n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    \"\"\"\n",
    "    # 1、构造请求\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    # print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        mblog = card['mblog']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        print(sina_text)\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、批量抓取微博\n",
    "\n",
    "在我们提取一条微博之后，就可以批量抓取微博了，如何批量？当然是要分页了！那微博是如何进行分页的呢？\n",
    "\n",
    "> 查找分页参数技巧：比较第一次和第二次的请求url，看看有何不同，找出不同的参数！可以使用一款文本比较工具：Beyond Compare\n",
    "\n",
    "比较两次请求的URL发现，第二次比第一次请求的连接中多了一个：`since_id`参数，而这个since_id参数就是每条微博的id。\n",
    "\n",
    "> 微博的分页机制：根据时间分页，每一条微博都有一个since_id,时间越大的since_id越大所以在请求的时候将since_id传入，依次请求，这样便能实现分页。\n",
    "\n",
    "了解微博的分页机制之后，就可以定制分页策略：`将上一次请求返回的微博中最小的since_id作为下次请求的参数，这样就等于根据时间倒序分页抓取数据！`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-d8cabe0e044c>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-d8cabe0e044c>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    #         spider_topic()\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "# 每次请求中最小的since_id，下次请求使用，新浪分页机制\n",
    "min_since_id = ''\n",
    "\n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    新浪微博分页机制：根据时间分页，每一条微博都有一个since_id，时间越大的since_id越大\n",
    "    所以在请求的时候将since_id传入，则会加载对应的话题下比此since_id小的微博，然后又重新获取最小的since_id\n",
    "    将最小的since_id传入，依次请求。这样就能实现分页\n",
    "    \"\"\"\n",
    "    global min_since_id\n",
    "    # 1、构造请求\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    # 如果存在min_since_id，就要加上\n",
    "    if min_since_id:\n",
    "        topic_url = topic_url + '&since_id=' + min_since_id\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        mblog = card['mblog']\n",
    "        # 解析微博的内容,可以知道mblog下的['id']的值就是r_since_id\n",
    "        r_since_id = mblog['id']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        print(sina_text)\n",
    "        # 3、获得最小的since_id，下次请求的时候使用\n",
    "        # 设置一个全局变量，然后获取最小的since_id,下次请求把它传入，则会请求比这个since_id小的微博，也就是比那条微博早的数据。\n",
    "        if min_since_id:\n",
    "            min_since_id = r_since_id if min_since_id > r_since_id else min_since_id\n",
    "        else:\n",
    "            min_since_id = r_since_id\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 批量爬取\n",
    "#     for i in range(1000):\n",
    "#         print('第%d页' % (i + 1))\n",
    "#         spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后写一个for循环调用上面的那个方法即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量爬取\n",
    "for i in range(1000):\n",
    "    print('第%d页' % (i + 1))\n",
    "    spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、爬取用户信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量爬取微博搞定之后，就可以爬取用户信息。\n",
    "\n",
    "首先我们得了解用户的基本信息页面的链接为：https://weibo.cn/5854XXX476/info ,需要使用低版本的微博页面，这样抓取信息会简单。\n",
    "\n",
    "只要获取到用户的id就可以拿到她的公开信息\n",
    "\n",
    "### 1、获取用户的id\n",
    "回到之前的浏览器分析页面分析微博的数据格式，发现其中的就有我们先要的用户id！\n",
    "\n",
    "所以我们需要将微博的内容提取出来时候可以顺便将用户的id提取出来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "# 每次请求中最小的since_id，下次请求使用，新浪分页机制\n",
    "min_since_id = ''\n",
    "\n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    新浪微博分页机制：根据时间分页，每一条微博都有一个since_id，时间越大的since_id越大\n",
    "    所以在请求的时候将since_id传入，则会加载对应的话题下比此since_id小的微博，然后又重新获取最小的since_id\n",
    "    将最小的since_id传入，依次请求。这样就能实现分页\n",
    "    \"\"\"\n",
    "    # 1、构造请求\n",
    "    global min_since_id\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    # 如果存在min_since_id，就要加上\n",
    "    if min_since_id:\n",
    "        topic_url = topic_url + '&since_id=' + min_since_id\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        mblog = card['mblog']\n",
    "        # 5.1 获取并解析用户的信息\n",
    "        user = mblog['user']\n",
    "        # 获取用户的id\n",
    "        user_id = user['id']\n",
    "        print(user_id)\n",
    "        # 解析微博的内容,可以知道mblog下的['id']的值就是r_since_id\n",
    "        r_since_id = mblog['id']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        print(sina_text)\n",
    "        # 3、获得最小的since_id，下次请求的时候使用\n",
    "        # 设置一个全局变量，然后获取最小的since_id,下次请求把它传入，则会请求比这个since_id小的微博，也就是比那条微博早的数据。\n",
    "        if min_since_id:\n",
    "            min_since_id = r_since_id if min_since_id > r_since_id else min_since_id\n",
    "        else:\n",
    "            min_since_id = r_since_id\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、模拟登陆\n",
    "\n",
    "当获取到用户的id之后，只要请求:https://weibo.cn/用户id/info 这个url就可以获取到用户的公开信息了，但是查看别人的主页信息是需要登录的，这样我们就可以先用代码模拟登陆。\n",
    "\n",
    "我们使用的登录接口是：https://passport.sina.cn/sso/login ，这个是2/3G手机的对应的按键机器的登录界面。\n",
    "\n",
    "登录时候会使用到requests.Session()对象，这个对象会自动保存cookies，下次请求自动带上cookies！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-98edd0124eb0>, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-98edd0124eb0>\"\u001b[0;36m, line \u001b[0;32m92\u001b[0m\n\u001b[0;31m    # spider_topic()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "# 每次请求中最小的since_id，下次请求使用，新浪分页机制\n",
    "min_since_id = ''\n",
    "\n",
    "def login_sina():\n",
    "    \"\"\"\n",
    "    登录新浪\n",
    "    \"\"\"\n",
    "    # 登录url\n",
    "    login_url = 'https://passport.sina.cn/sso/login'\n",
    "    # 请求头设置\n",
    "    headers = {\n",
    "        'Referer': 'https://sina.cn/index/settings?vt=4&pos=108',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'\n",
    "    }\n",
    "    # 使用用户名和密码\n",
    "    data = {\n",
    "        'username':'用户名',\n",
    "        'password':'密码',\n",
    "        'savestate':1,\n",
    "        'entry':'mweibo',\n",
    "        'mainpageflag':1\n",
    "    }\n",
    "    try:\n",
    "        r = s.post(login_url, headers=headers, data=data)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('登录请求失败')\n",
    "        return 0\n",
    "    # 打印请求结果\n",
    "    print(json.loads(r.text)['msg'])\n",
    "    return 1\n",
    "    \n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    新浪微博分页机制：根据时间分页，每一条微博都有一个since_id，时间越大的since_id越大\n",
    "    所以在请求的时候将since_id传入，则会加载对应的话题下比此since_id小的微博，然后又重新获取最小的since_id\n",
    "    将最小的since_id传入，依次请求。这样就能实现分页\n",
    "    \"\"\"\n",
    "    # 1、构造请求\n",
    "    global min_since_id\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    # 如果存在min_since_id，就要加上\n",
    "    if min_since_id:\n",
    "        topic_url = topic_url + '&since_id=' + min_since_id\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        mblog = card['mblog']\n",
    "        # 5.1 获取并解析用户的信息\n",
    "        user = mblog['user']\n",
    "        # 获取用户的id\n",
    "        user_id = user['id']\n",
    "        print(user_id)\n",
    "        # 解析微博的内容,可以知道mblog下的['id']的值就是r_since_id\n",
    "        r_since_id = mblog['id']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        print(sina_text)\n",
    "        # 3、获得最小的since_id，下次请求的时候使用\n",
    "        # 设置一个全局变量，然后获取最小的since_id,下次请求把它传入，则会请求比这个since_id小的微博，也就是比那条微博早的数据。\n",
    "        if min_since_id:\n",
    "            min_since_id = r_since_id if min_since_id > r_since_id else min_since_id\n",
    "        else:\n",
    "            min_since_id = r_since_id\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、抓取用户公开信息\n",
    "\n",
    "拿到用户的id又登录之后，就可以开始抓取用户的公开信息了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_user_info(uid)->list:\n",
    "    \"\"\"\n",
    "    爬取用户的信息（需要登录），并将基本的信息解析成列表返回\n",
    "    : return: ['用户名', '性别', '地区', '生日']\n",
    "    \"\"\"\n",
    "    user_info_url = 'https://weibo.cn/%s/info' % uid\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/status/info?jumpfrom=wapv4&tip=1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=user_info_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取用户信息失败')\n",
    "        return\n",
    "    # 使用正则提取基本信息\n",
    "    basic_info_html = re.findall('<div class=\"tip\">基本信息</div><div class=\"c\">(.*?)</div>', r.text)\n",
    "    # 提取：用户名、性别、地区、生日 这些信息\n",
    "    basic_infos = get_basic_info_list(basic_info_html)\n",
    "    return basic_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里公开的信息我们只要：用户名、性别、地区、生日这些数据！所以我们需要将这几个数据提取出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_info_list(basic_info_html)->list:\n",
    "    \"\"\"\n",
    "    将HTML解析提取需要的字段\n",
    "    :param basic_info_html:\n",
    "    :return:['用户名', '性别', '地区', '生日']\n",
    "    \"\"\"\n",
    "    basic_infos = []\n",
    "    basic_info_kvs = basic_info_html[0].split('<br/>')\n",
    "    print(basic_info_kvs)\n",
    "    # 遍历每个传入的数据\n",
    "    for basic_info_kv in basic_info_kvs:\n",
    "        if basic_info_kv.startswith('昵称'):\n",
    "            basic_infos.append(basic_info_kv.split(':')[1])\n",
    "        elif basic_info_kv.startswith('性别'):\n",
    "            basic_infos.append(basic_info_kv.split(':')[1])\n",
    "        elif basic_info_kv.startswith('地区'):\n",
    "            area = basic_info_kv.split(':')[1]\n",
    "            # 如果地区是其他的话，就添加空\n",
    "            if '其他' in area or '海外' in area:\n",
    "                basic_infos.append('')\n",
    "                continue\n",
    "            # 浙江 杭州，这里只要省（按照省份分，为后面做准备）\n",
    "            if ' ' in area:\n",
    "                area = area.split(' ')[0]\n",
    "            basic_infos.append(area)\n",
    "        elif basic_info_kv.startswith('生日'):\n",
    "            birthday = basic_info_kv.split(':')[1]\n",
    "            # 19xx 年和20xx带年份的才有效，只有日月或者星座的数据无效\n",
    "            if birthday.startswith('19') or birthday.startswith('20'):\n",
    "                # 只要前3位，如198,199,200分别代表80后，90后和00后，方便后面的数据分析\n",
    "                basic_infos.append(birthday[:3])\n",
    "            else:\n",
    "                basic_infos.append('')\n",
    "        else:\n",
    "            pass\n",
    "    # 有些用户的生日是没有的，所以直接添加一个空字符\n",
    "    if len(basic_infos) < 4:\n",
    "        basic_infos.append('')\n",
    "    return basic_infos\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抓取用户的信息不能太过于频繁，否则会出现请求失败（响应码为418），但是不会封IP，其实很多大厂不会轻易的封ip，容易误伤，也去一封就是一个小区甚至是更大！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、保存csv文件\n",
    "微博信息拿到，用户信息拿到了，就要把这些数据保存起来，方便后面做数据分析！\n",
    "\n",
    "之前一直保存的数据是txt格式的，因为都只是一项数据，而这次的是多项数据（微博内容，用户名，地区，年龄和性别等），所以选择CSV（Comma Separated Values逗号分隔值）格式的文件！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "# 每次请求中最小的since_id，下次请求使用，新浪分页机制\n",
    "min_since_id = ''\n",
    "\n",
    "def login_sina():\n",
    "    \"\"\"\n",
    "    登录新浪\n",
    "    \"\"\"\n",
    "    # 登录url\n",
    "    login_url = 'https://passport.sina.cn/sso/login'\n",
    "    # 请求头设置\n",
    "    headers = {\n",
    "        'Referer': 'https://sina.cn/index/settings?vt=4&pos=108',\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'\n",
    "    }\n",
    "    # 使用用户名和密码\n",
    "    data = {\n",
    "        'username':'用户名',\n",
    "        'password':'密码',\n",
    "        'savestate':1,\n",
    "        'entry':'mweibo',\n",
    "        'mainpageflag':1\n",
    "    }\n",
    "    try:\n",
    "        r = s.post(login_url, headers=headers, data=data)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('登录请求失败')\n",
    "        return 0\n",
    "    # 打印请求结果\n",
    "    print(json.loads(r.text)['msg'])\n",
    "    return 1\n",
    "    \n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    新浪微博分页机制：根据时间分页，每一条微博都有一个since_id，时间越大的since_id越大\n",
    "    所以在请求的时候将since_id传入，则会加载对应的话题下比此since_id小的微博，然后又重新获取最小的since_id\n",
    "    将最小的since_id传入，依次请求。这样就能实现分页\n",
    "    \"\"\"\n",
    "    # 1、构造请求\n",
    "    global min_since_id\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    # 如果存在min_since_id，就要加上\n",
    "    if min_since_id:\n",
    "        topic_url = topic_url + '&since_id=' + min_since_id\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        # 六、创建保存数据的列表，最后将它写入到csv文件（对应本栏保存）\n",
    "        sina_columns = []\n",
    "        mblog = card['mblog']\n",
    "        # 5.1 获取并解析用户的信息\n",
    "        user = mblog['user']\n",
    "        # 爬取用户信息，微博有反扒机制，频率太快的请求返回418\n",
    "        try:\n",
    "            basic_infos = spider_user_info(user['id'])\n",
    "        except:\n",
    "            print('用户信息爬取失败! id=%s' % user['id'])\n",
    "            continue\n",
    "        # 把用户信息放入列表（对应本栏保存）,空表压栈，列表扩展\n",
    "        sina_columns.append(user['id'])\n",
    "        sina_columns.extend(basic_infos)\n",
    "        # 解析微博的内容,可以知道mblog下的['id']的值就是r_since_id\n",
    "        r_since_id = mblog['id']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        # 将微博内容写入列表\n",
    "        sina_columns.append(r_since_id)\n",
    "        sina_columns.append(sina_text)\n",
    "        print(sina_columns)\n",
    "        # 检验列表中的信息是否完整（对应本栏保存）\n",
    "        # sina_columns数据格式：['用户id','用户名','性别','地区','生日','微博ID','微博内容']\n",
    "        if len(sina_columns) < 7:\n",
    "            print('-------上一条数据内容不完整----------')\n",
    "            continue\n",
    "        \n",
    "        # 保存数据（对应本栏保存）\n",
    "        save_columns_to_csv(sina_columns)\n",
    "        \n",
    "        # 获得最小的since_id，下次请求的时候使用\n",
    "        # 设置一个全局变量，然后获取最小的since_id,下次请求把它传入，则会请求比这个since_id小的微博，也就是比那条微博早的数据。\n",
    "        if min_since_id:\n",
    "            min_since_id = r_since_id if min_since_id > r_since_id else min_since_id\n",
    "        else:\n",
    "            min_since_id = r_since_id\n",
    "            \n",
    "        # 爬取用户信息不能太频繁，所以需要设置一个时间间隔：3-6秒内\n",
    "        time.sleep(random.randint(3, 6))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    spider_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们生成一个列表，然后将数据按照顺序放入，再写入CSV文件！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_columns_to_csv(columns, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    将数据保存到CSV文件中\n",
    "    数据格式为：'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    sina_columns数据格式：['用户id','用户名','性别','地区','生日','微博ID','微博内容']\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'a', encoding=encoding) as csvfile:\n",
    "        csv_write = csv.writer(csvfile)\n",
    "        csv_write.writerow(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合上面的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-dd7f84ad05d1>, line 213)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-dd7f84ad05d1>\"\u001b[0;36m, line \u001b[0;32m213\u001b[0m\n\u001b[0;31m    # patch_spider_topic()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 生成Session对象，用于保存Cookie\n",
    "s = requests.Session()\n",
    "# 每次请求中最小的since_id，下次请求使用，新浪分页机制\n",
    "min_since_id = ''\n",
    "# 新浪话题数据保存文件\n",
    "CSV_FILE_PATH = 'sina_topic.csv'\n",
    "\n",
    "def login_sina():\n",
    "    \"\"\"\n",
    "    登录新浪\n",
    "    \"\"\"\n",
    "#     # 登录url\n",
    "#     login_url = 'https://passport.weibo.cn/sso/login'\n",
    "#     # 请求头设置\n",
    "#     headers = {\n",
    "#         'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&res=wel&wm=3349&r=https%3A%2F%2Fm.weibo.cn%2F',\n",
    "#         'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'\n",
    "#     }\n",
    "    # 登录URL\n",
    "    login_url = 'https://passport.weibo.cn/sso/login'\n",
    "    # 请求头\n",
    "    headers = {'user-agent': 'Mozilla/5.0',\n",
    "               'Referer': 'https://passport.weibo.cn/signin/login?entry=mweibo&res=wel&wm=3349&r=https%3A%2F%2Fm.weibo.cn%2F'}\n",
    "\n",
    "    # 使用用户名和密码\n",
    "    data = {\n",
    "        'username':'15218218255',\n",
    "        'password':'Wb13414851554',\n",
    "        'savestate':1,\n",
    "        'entry':'mweibo',\n",
    "        'mainpageflag':1\n",
    "    }\n",
    "    try:\n",
    "        r = s.post(login_url, headers=headers, data=data)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('登录请求失败')\n",
    "        return 0\n",
    "    # 打印请求结果\n",
    "    print(json.loads(r.text)['msg'])\n",
    "    return 1\n",
    "\n",
    "    \n",
    "def spider_topic():\n",
    "    \"\"\"\n",
    "    获取新浪话题\n",
    "    新浪微博分页机制：根据时间分页，每一条微博都有一个since_id，时间越大的since_id越大\n",
    "    所以在请求的时候将since_id传入，则会加载对应的话题下比此since_id小的微博，然后又重新获取最小的since_id\n",
    "    将最小的since_id传入，依次请求。这样就能实现分页\n",
    "    \"\"\"\n",
    "    # 1、构造请求\n",
    "    global min_since_id\n",
    "    topic_url = 'https://m.weibo.cn/api/container/getIndex?containerid=1008087a8941058aaf4df5147042ce104568da_-_feed'\n",
    "    # 如果存在min_since_id，就要加上\n",
    "    if min_since_id:\n",
    "        topic_url = topic_url + '&since_id=' + min_since_id\n",
    "    headers = {\n",
    "        'Referer': 'https://m.weibo.cn/p/index?containerid=1008087a8941058aaf4df5147042ce104568da&extparam=%E5%91%A8%E6%9D%B0%E4%BC%A6&luicode=10000011&lfid=100103type%3D61%26q%3D%E5%91%A8%E6%9D%B0%E4%BC%A6%E8%B6%85%E8%AF%9D%26t%3D0&sudaref=m.weibo.cn&display=0&retcode=6102',\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "    }\n",
    "    try:\n",
    "        r = s.get(url=topic_url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取失败！')\n",
    "        return\n",
    "    # print(r.text)\n",
    "    # 2、解析数据\n",
    "    r_json = json.loads(r.text)\n",
    "    cards = r_json['data']['cards']\n",
    "    # 2.1、第一次请求cards包含的微博和头部信息，以后请求返回的只有微博信息,所以要从第2个开始。\n",
    "    card_group = cards[2]['card_group'] if len(cards) > 1 else cards[0]['card_group']\n",
    "    for card in card_group:\n",
    "        # 六、创建保存数据的列表，最后将它写入到csv文件（对应本栏保存）\n",
    "        sina_columns = []\n",
    "        mblog = card['mblog']\n",
    "        # 5.1 获取并解析用户的信息\n",
    "        user = mblog['user']\n",
    "        # 爬取用户信息，微博有反扒机制，频率太快的请求返回418\n",
    "        try:\n",
    "            basic_infos = spider_user_info(user['id'])\n",
    "        except:\n",
    "            print('用户信息爬取失败! id=%s' % user['id'])\n",
    "            continue\n",
    "        # 把用户信息放入列表（对应本栏保存）,空表压栈，列表扩展\n",
    "        sina_columns.append(user['id'])\n",
    "        sina_columns.extend(basic_infos)\n",
    "        # 解析微博的内容,可以知道mblog下的['id']的值就是r_since_id\n",
    "        r_since_id = mblog['id']\n",
    "        # 过滤html标签，留下内容\n",
    "        sina_text = re.compile(r'<[^>]+>',re.S).sub(' ', mblog['text'])\n",
    "        # 除去无用的开头信息\n",
    "        sina_text = sina_text.replace('周杰伦超话', '').strip()\n",
    "        # 将微博内容写入列表\n",
    "        sina_columns.append(r_since_id)\n",
    "        sina_columns.append(sina_text)\n",
    "        print(sina_columns)\n",
    "        # 检验列表中的信息是否完整（对应本栏保存）\n",
    "        # sina_columns数据格式：['用户id','用户名','性别','地区','生日','微博ID','微博内容']\n",
    "        if len(sina_columns) < 7:\n",
    "            print('-------上一条数据内容不完整----------')\n",
    "            continue\n",
    "        \n",
    "        # 保存数据（对应本栏保存）\n",
    "        save_columns_to_csv(sina_columns)\n",
    "        \n",
    "        # 获得最小的since_id，下次请求的时候使用\n",
    "        # 设置一个全局变量，然后获取最小的since_id,下次请求把它传入，则会请求比这个since_id小的微博，也就是比那条微博早的数据。\n",
    "        if min_since_id:\n",
    "            min_since_id = r_since_id if min_since_id > r_since_id else min_since_id\n",
    "        else:\n",
    "            min_since_id = r_since_id\n",
    "            \n",
    "        # 爬取用户信息不能太频繁，所以需要设置一个时间间隔：3-6秒内\n",
    "        time.sleep(random.randint(3, 6))\n",
    "\n",
    "\n",
    "def spider_user_info(uid) -> list:\n",
    "    \"\"\"\n",
    "    爬取用户的信息（需要登录），并将基本的信息解析成列表返回\n",
    "    : return: ['用户名', '性别', '地区', '生日']\n",
    "    \"\"\"\n",
    "    user_info_url = 'https://weibo.cn/%s/info' % uid\n",
    "#     headers = {\n",
    "#         # 'Referer': 'https://m.weibo.cn/status/info?jumpfrom=wapv4&tip=1',\n",
    "#         'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'\n",
    "#     }\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "        r = s.get(url=user_info_url, headers=headers) # 要细心！\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        print('爬取用户信息失败')\n",
    "        return\n",
    "    # 使用正则提取基本信息\n",
    "    basic_info_html = re.findall('<div class=\"tip\">基本信息</div><div class=\"c\">(.*?)</div>', r.text)\n",
    "    # 提取：用户名、性别、地区、生日 这些信息\n",
    "    basic_infos = get_basic_info_list(basic_info_html)\n",
    "    return basic_infos\n",
    "\n",
    "def get_basic_info_list(basic_info_html)->list:\n",
    "    \"\"\"\n",
    "    将HTML解析提取需要的字段\n",
    "    :param basic_info_html:\n",
    "    :return:['用户名', '性别', '地区', '生日']\n",
    "    \"\"\"\n",
    "    basic_infos = []\n",
    "    basic_info_kvs = basic_info_html[0].split('<br/>')\n",
    "    print(basic_info_kvs)\n",
    "    # 遍历每个传入的数据\n",
    "    for basic_info_kv in basic_info_kvs:\n",
    "        if basic_info_kv.startswith('昵称'):\n",
    "            basic_infos.append(basic_info_kv.split(':')[1])\n",
    "        elif basic_info_kv.startswith('性别'):\n",
    "            basic_infos.append(basic_info_kv.split(':')[1])\n",
    "        elif basic_info_kv.startswith('地区'):\n",
    "            area = basic_info_kv.split(':')[1]\n",
    "            # 如果地区是其他的话，就添加空\n",
    "            if '其他' in area or '海外' in area:\n",
    "                basic_infos.append('')\n",
    "                continue\n",
    "            # 浙江 杭州，这里只要省（按照省份分，为后面做准备）\n",
    "            if ' ' in area:\n",
    "                area = area.split(' ')[0]\n",
    "            basic_infos.append(area)\n",
    "        elif basic_info_kv.startswith('生日'):\n",
    "            birthday = basic_info_kv.split(':')[1]\n",
    "            # 19xx 年和20xx带年份的才有效，只有日月或者星座的数据无效\n",
    "            if birthday.startswith('19') or birthday.startswith('20'):\n",
    "                # 只要前3位，如198,199,200分别代表80后，90后和00后，方便后面的数据分析\n",
    "                basic_infos.append(birthday[:3])\n",
    "            else:\n",
    "                basic_infos.append('')\n",
    "        else:\n",
    "            pass\n",
    "    # 有些用户的生日是没有的，所以直接添加一个空字符\n",
    "    if len(basic_infos) < 4:\n",
    "        basic_infos.append('')\n",
    "    return basic_infos\n",
    "        \n",
    "\n",
    "def save_columns_to_csv(columns, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    将数据保存到CSV文件中\n",
    "    数据格式为：'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    sina_columns数据格式：['用户id','用户名','性别','地区','生日','微博ID','微博内容']\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'a', encoding=encoding) as csvfile:\n",
    "        csv_write = csv.writer(csvfile)\n",
    "        csv_write.writerow(columns)\n",
    "\n",
    "def patch_spider_topic():\n",
    "    # 爬取前先登录，登录失败则不爬取\n",
    "    if not login_sina():\n",
    "        return\n",
    "    # 写入数据之前先清空之前的数据\n",
    "    if os.path.exists(CSV_FILE_PATH):\n",
    "        os.remove(CSV_FILE_PATH)\n",
    "    # 批量爬取数据\n",
    "    for i in range(1000):\n",
    "        print('第%d页' %(i+1))\n",
    "        spider_topic()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # patch_spider_topic()  # 数据抓取速度是个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、数据分析\n",
    "数据保存下来之后我们就可以进行数据分析了，首先我们需要知道我们需要进行分析哪些数据：\n",
    "- 1、我们可以将性别数据做成饼图，简单直观\n",
    "- 2、将年龄数据做出柱状图，方便对比，看看到底是不是夕阳红老年团\n",
    "- 3、将地区做成中国的热力图，看看那个地区的粉丝最为活跃\n",
    "- 4、最后将微博内容做成词云图，直观了解大家都在说什么\n",
    "\n",
    "### 1、读取CSV文件\n",
    "\n",
    "因为我们保存的数据格式为:“用户id”，“用户名”，“性别”，“地区”，“生日”，“微博id”，“微博内容”的很多行，而现在做数据分析需要获取指定的某一列，比如：性别列，所以我们需要封装一个方法用来读取指定的列！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dict(index) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV数据\n",
    "    数据格式为:'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    :param index:读取某一列从0开始\n",
    "    :return : dic属性为key，次数为value\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [column[index] for columns in reader]\n",
    "        dic = collections.Counter(column)\n",
    "        # 删除空字符串\n",
    "        if '' in dict:\n",
    "            dic.pop('')\n",
    "        print(dic)\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的代码可以知道使用了`Counter`类来统计词频，方便后面数据分析，他返回的格式为：{'女':1062, '男': 637}.\n",
    "\n",
    "### 2、可视化库pyecharts\n",
    "在开始进行数据分析之前，有一件很重要的事情，那就是选择一个合适的可视化库。Python的可视化库非常多，使用`matplotlib库`可以做词云，也可以做一些简单的绘图，非常方便。这里我们需要做一个全国分布图，通过筛选，可以选择国人开发的pyecharts。选择这个库的理由是：开源免费、文档详细、图形丰富、代码简介，方便。\n",
    "- 官网：https://pyecharts.org/#/\n",
    "\n",
    "- 源码：https://github.com/pyecharts/pyecharts\n",
    "\n",
    "- 安装：sudo pip3 install pyecharts\n",
    "\n",
    "### 3、分析性别\n",
    "使用可视化库之后，代码编写如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'女': 1580, '男': 939})\n",
      "[['女', 1580], ['男', 939]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections # 会使用集合中的Counter\n",
    "\n",
    "import jieba.analyse\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "from pyecharts.charts import Pie #, Bar, Map,WordCloud # 饼图、柱状图、地图、云词\n",
    "\n",
    "# 新浪话题数据保存文件\n",
    "CSV_FILE_PATH = 'sina_topic.csv'\n",
    "# 需要清洗的词\n",
    "STOP_WORDS_FILE_PATH = 'stop_words.txt'\n",
    "\n",
    "def read_csv_to_dict(index) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV数据\n",
    "    数据格式为:'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    :param index:读取某一列从0开始\n",
    "    :return : dic属性为key，次数为value\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [columns[index] for columns in reader]\n",
    "        # print(column)  # 打印出传入参数index对应的那一列数据\n",
    "        dic = collections.Counter(column)\n",
    "        # 删除空字符串\n",
    "        if '' in dic:\n",
    "            dic.pop('')\n",
    "        print(dic)\n",
    "        return dic\n",
    "\n",
    "def analysis_gender():\n",
    "    \"\"\"\n",
    "    分析性别\n",
    "    \"\"\"\n",
    "    # 读取性别列,第二列\n",
    "    dic = read_csv_to_dict(2)\n",
    "    # 生成二维数组\n",
    "    gender_count_list = [list(z) for z in zip(dic.keys(), dic.values())]\n",
    "    print(gender_count_list)\n",
    "    pie = (\n",
    "        Pie()\n",
    "            .add(\"\", gender_count_list)\n",
    "            .set_colors([\"red\", \"blue\"])\n",
    "            .set_global_opts(title_opts=opts.TitleOpts(title=\"性别分析\"))\n",
    "            .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}:{c}\"))\n",
    "    )\n",
    "    pie.render('gender.html')\n",
    "    pie.render_notebook()  # 或者直接在notebook中显示。(出了问题，显示不出来)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    analysis_gender()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的可以是HTML，也可以直接在notebook里面显示，因为notebook强大之处就在于数据分析。当然也可以在浏览器打开html。\n",
    "\n",
    "\n",
    "### 4、分析年龄\n",
    "使用柱状图和饼图进行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'199': 935, '198': 161, '200': 146, '201': 64, '197': 21, '190': 19, '191': 3, '194': 2, '195': 1, '196': 1})\n",
      "{'190': 19, '191': 3, '194': 2, '195': 1, '196': 1, '197': 21, '198': 161, '199': 935, '200': 146, '201': 64}\n",
      "[['197', 21], ['199', 935], ['200', 146], ['201', 64], ['190', 19], ['198', 161], ['195', 1], ['196', 1], ['191', 3], ['194', 2]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections # 会使用集合中的Counter\n",
    "\n",
    "import jieba.analyse\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "from pyecharts.charts import Pie, Bar, Map,WordCloud # 饼图、柱状图、地图、云词\n",
    "\n",
    "# 新浪话题数据保存文件\n",
    "CSV_FILE_PATH = 'sina_topic.csv'\n",
    "# 需要清洗的词\n",
    "STOP_WORDS_FILE_PATH = 'stop_words.txt'\n",
    "\n",
    "def read_csv_to_dict(index) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV数据\n",
    "    数据格式为:'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    :param index:读取某一列从0开始\n",
    "    :return : dic属性为key，次数为value\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [columns[index] for columns in reader]\n",
    "        # print(column)  # 打印出传入参数index对应的那一列数据\n",
    "        dic = collections.Counter(column)\n",
    "        # 删除空字符串\n",
    "        if '' in dic:\n",
    "            dic.pop('')\n",
    "        print(dic)\n",
    "        return dic\n",
    "\n",
    "def analysis_age():\n",
    "    \"\"\"\n",
    "    分析年龄\n",
    "    \"\"\"\n",
    "    # 读取性别列,第四列,获取到的是年龄的列表\n",
    "    dic = read_csv_to_dict(4)\n",
    "    # 生成柱状图\n",
    "    sorted_dic = {}\n",
    "    for key in sorted(dic):\n",
    "        sorted_dic[key] = dic[key]\n",
    "    print(sorted_dic)\n",
    "    bar = (\n",
    "        Bar()\n",
    "            .add_xaxis(list(sorted_dic.keys()))\n",
    "            .add_yaxis(\"周杰伦打榜粉丝年龄分析\", list(sorted_dic.values()))\n",
    "            .set_global_opts(\n",
    "            yaxis_opts = opts.AxisOpts(name=\"数量\"),\n",
    "            xaxis_opts = opts.AxisOpts(name=\"年龄\"),\n",
    "            )\n",
    "    )\n",
    "    bar.render('age_bar.html')\n",
    "    bar.render_notebook()\n",
    "    # 生成饼图\n",
    "    age_count_list = [list(z) for z in zip(dic.keys(), dic.values())]\n",
    "    print(age_count_list)\n",
    "    pie = (\n",
    "        Pie()\n",
    "            .add(\"\", age_count_list)\n",
    "            .set_global_opts(title_opts=opts.TitleOpts(title=\"周杰伦打榜粉丝年龄分析\"))\n",
    "            .set_series_opts(label_opts=opts.LabelOpts(formatter=\"{b}:{c}\"))\n",
    "    )\n",
    "    pie.render('age-pie.html')\n",
    "    # pie.render_notebook()  # 或者直接在notebook中显示。(出了问题，显示不出来)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    analysis_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、分析地区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'广东': 196, '北京': 153, '浙江': 138, '江苏': 126, '上海': 122, '山东': 99, '四川': 93, '湖北': 87, '河南': 84, '陕西': 58, '福建': 58, '重庆': 56, '湖南': 55, '安徽': 53, '河北': 47, '辽宁': 47, '黑龙江': 41, '江西': 36, '云南': 31, '山西': 29, '天津': 28, '吉林': 28, '内蒙古': 27, '新疆': 25, '广西': 22, '台湾': 19, '贵州': 18, '甘肃': 15, '海南': 12, '香港': 8, '宁夏': 8, '西藏': 6, '青海': 3, '澳门': 2})\n",
      "[['安徽', 53], ['台湾', 19], ['广东', 196], ['湖北', 87], ['上海', 122], ['北京', 153], ['河南', 84], ['海南', 12], ['内蒙古', 27], ['新疆', 25], ['浙江', 138], ['山东', 99], ['河北', 47], ['黑龙江', 41], ['四川', 93], ['重庆', 56], ['江苏', 126], ['贵州', 18], ['湖南', 55], ['江西', 36], ['陕西', 58], ['天津', 28], ['辽宁', 47], ['福建', 58], ['香港', 8], ['山西', 29], ['云南', 31], ['吉林', 28], ['甘肃', 15], ['西藏', 6], ['青海', 3], ['广西', 22], ['宁夏', 8], ['澳门', 2]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections # 会使用集合中的Counter\n",
    "\n",
    "import jieba.analyse\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "from pyecharts.charts import Pie #, Bar, Map,WordCloud # 饼图、柱状图、地图、云词\n",
    "\n",
    "# 新浪话题数据保存文件\n",
    "CSV_FILE_PATH = 'sina_topic.csv'\n",
    "# 需要清洗的词\n",
    "STOP_WORDS_FILE_PATH = 'stop_words.txt'\n",
    "\n",
    "def read_csv_to_dict(index) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV数据\n",
    "    数据格式为:'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    :param index:读取某一列从0开始\n",
    "    :return : dic属性为key，次数为value\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [columns[index] for columns in reader]\n",
    "        # print(column)  # 打印出传入参数index对应的那一列数据\n",
    "        dic = collections.Counter(column)\n",
    "        # 删除空字符串\n",
    "        if '' in dic:\n",
    "            dic.pop('')\n",
    "        print(dic)\n",
    "        return dic\n",
    "\n",
    "def analysis_area():\n",
    "    \"\"\"\n",
    "    分析地区\n",
    "    :return\n",
    "    \"\"\"\n",
    "    # 读取地区列,第三列\n",
    "    dic = read_csv_to_dict(3)\n",
    "    # 生成二维数组\n",
    "    area_count_list = [list(z) for z in zip(dic.keys(), dic.values())]\n",
    "    print(area_count_list)\n",
    "    map = (\n",
    "        Map()\n",
    "            .add(\"周杰伦打榜粉丝地区分析\", area_count_list, \"china\")\n",
    "            .set_global_opts(\n",
    "            visualmap_opts = opts.VisualMapOpts(max_=200)\n",
    "            )\n",
    "            \n",
    "    )\n",
    "    map.render('area.html')\n",
    "    map.render_notebook()  # 或者直接在notebook中显示。(出了问题，显示不出来)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    analysis_area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、打榜内容分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections # 会使用集合中的Counter\n",
    "\n",
    "import jieba.analyse\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "from pyecharts.charts import Pie, Bar, Map,WordCloud # 饼图、柱状图、地图、云词\n",
    "\n",
    "# 新浪话题数据保存文件\n",
    "CSV_FILE_PATH = 'sina_topic.csv'\n",
    "# 需要清洗的词\n",
    "STOP_WORDS_FILE_PATH = 'stop_words.txt'\n",
    "\n",
    "def read_csv_to_dict(index) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV数据\n",
    "    数据格式为:'用户id','用户名','性别','地区','生日','微博id','微博内容'\n",
    "    :param index:读取某一列从0开始\n",
    "    :return : dic属性为key，次数为value\n",
    "    \"\"\"\n",
    "    with open(CSV_FILE_PATH, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        column = [columns[index] for columns in reader]\n",
    "        # print(column)  # 打印出传入参数index对应的那一列数据\n",
    "        dic = collections.Counter(column)\n",
    "        # 删除空字符串\n",
    "        if '' in dic:\n",
    "            dic.pop('')\n",
    "        print(dic)\n",
    "        return dic\n",
    "\n",
    "def analysis_sina_content():\n",
    "    \"\"\"\n",
    "    分析微博内容\n",
    "    :return\n",
    "    \"\"\"\n",
    "    # 读取微博内容列,第六列\n",
    "    dic = read_csv_to_dict(6)\n",
    "    # 数据清洗，去掉无效的词\n",
    "    jieba.analyse.set_stop_words(STOP_WORDS_FILE_PATH)\n",
    "    # 词数统计\n",
    "    words_count_list = jieba.analyse.textrank(' '.join(dic.keys()), topK=50, withWeight=True)\n",
    "    print(words_count_list)\n",
    "    # 生成词云\n",
    "    word_cloud = (\n",
    "        WordCloud()\n",
    "            .add(\"\", words_count_list, word_size_range=[20,100], shape=SymbolType.DIAMOND)\n",
    "            .set_global_opts(title_opts=opts.TitleOpts(title=\"周杰伦打榜微博内容分析\"))\n",
    "    )\n",
    "    word_cloud.render('word_cloud.html')\n",
    "    word_cloud.render_notebook()  # 或者直接在notebook中显示。(出了问题，显示不出来)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    analysis_sina_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文参考自：微信公众号：裸睡的猪：\n",
    "\n",
    "https://mp.weixin.qq.com/s?__biz=MzI0OTc0MzAwNA==&mid=2247485164&idx=1&sn=16ce62c9c45db6ff1c9b29433053bc2b&chksm=e98d951fdefa1c096998b088e57b5811e873c23ce6388971f1f3af61e1b653ff81e328a518d3&scene=0&xtrack=1&pass_ticket=Jja%2BK6y5X9TtvC3Es97lXWpejBD%2F2hDDf393MAo%2BQ%2BFdJ33JpHFm11%2BJ%2FwUGqXVr#rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    require.config({\n",
       "        paths: {\n",
       "            'echarts':'https://assets.pyecharts.org/assets/echarts.min'\n",
       "        }\n",
       "    });\n",
       "</script>\n",
       "\n",
       "    <div id=\"acaefe24e05a4155a09d02f10ecf539d\" style=\"width:900px; height:500px;\"></div>\n",
       "\n",
       "\n",
       "<script>\n",
       "    require(['echarts'], function(echarts) {\n",
       "        var chart_acaefe24e05a4155a09d02f10ecf539d = echarts.init(\n",
       "            document.getElementById('acaefe24e05a4155a09d02f10ecf539d'), 'white', {renderer: 'canvas'});\n",
       "        var option_acaefe24e05a4155a09d02f10ecf539d = {\n",
       "    \"animation\": true,\n",
       "    \"animationThreshold\": 2000,\n",
       "    \"animationDuration\": 1000,\n",
       "    \"animationEasing\": \"cubicOut\",\n",
       "    \"animationDelay\": 0,\n",
       "    \"animationDurationUpdate\": 300,\n",
       "    \"animationEasingUpdate\": \"cubicOut\",\n",
       "    \"animationDelayUpdate\": 0,\n",
       "    \"color\": [\n",
       "        \"#c23531\",\n",
       "        \"#2f4554\",\n",
       "        \"#61a0a8\",\n",
       "        \"#d48265\",\n",
       "        \"#749f83\",\n",
       "        \"#ca8622\",\n",
       "        \"#bda29a\",\n",
       "        \"#6e7074\",\n",
       "        \"#546570\",\n",
       "        \"#c4ccd3\",\n",
       "        \"#f05b72\",\n",
       "        \"#ef5b9c\",\n",
       "        \"#f47920\",\n",
       "        \"#905a3d\",\n",
       "        \"#fab27b\",\n",
       "        \"#2a5caa\",\n",
       "        \"#444693\",\n",
       "        \"#726930\",\n",
       "        \"#b2d235\",\n",
       "        \"#6d8346\",\n",
       "        \"#ac6767\",\n",
       "        \"#1d953f\",\n",
       "        \"#6950a1\",\n",
       "        \"#918597\"\n",
       "    ],\n",
       "    \"series\": [\n",
       "        {\n",
       "            \"type\": \"bar\",\n",
       "            \"name\": \"\\u5546\\u5bb6A\",\n",
       "            \"data\": [\n",
       "                114,\n",
       "                55,\n",
       "                27,\n",
       "                101,\n",
       "                125,\n",
       "                27,\n",
       "                105\n",
       "            ],\n",
       "            \"barCategoryGap\": \"20%\",\n",
       "            \"label\": {\n",
       "                \"show\": true,\n",
       "                \"position\": \"top\",\n",
       "                \"margin\": 8\n",
       "            }\n",
       "        },\n",
       "        {\n",
       "            \"type\": \"bar\",\n",
       "            \"name\": \"\\u5546\\u5bb6B\",\n",
       "            \"data\": [\n",
       "                57,\n",
       "                134,\n",
       "                137,\n",
       "                129,\n",
       "                145,\n",
       "                60,\n",
       "                49\n",
       "            ],\n",
       "            \"barCategoryGap\": \"20%\",\n",
       "            \"label\": {\n",
       "                \"show\": true,\n",
       "                \"position\": \"top\",\n",
       "                \"margin\": 8\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"legend\": [\n",
       "        {\n",
       "            \"data\": [\n",
       "                \"\\u5546\\u5bb6A\",\n",
       "                \"\\u5546\\u5bb6B\"\n",
       "            ],\n",
       "            \"selected\": {\n",
       "                \"\\u5546\\u5bb6A\": true,\n",
       "                \"\\u5546\\u5bb6B\": true\n",
       "            },\n",
       "            \"show\": true\n",
       "        }\n",
       "    ],\n",
       "    \"tooltip\": {\n",
       "        \"show\": true,\n",
       "        \"trigger\": \"item\",\n",
       "        \"triggerOn\": \"mousemove|click\",\n",
       "        \"axisPointer\": {\n",
       "            \"type\": \"line\"\n",
       "        },\n",
       "        \"textStyle\": {\n",
       "            \"fontSize\": 14\n",
       "        },\n",
       "        \"borderWidth\": 0\n",
       "    },\n",
       "    \"xAxis\": [\n",
       "        {\n",
       "            \"show\": true,\n",
       "            \"scale\": false,\n",
       "            \"nameLocation\": \"end\",\n",
       "            \"nameGap\": 15,\n",
       "            \"gridIndex\": 0,\n",
       "            \"inverse\": false,\n",
       "            \"offset\": 0,\n",
       "            \"splitNumber\": 5,\n",
       "            \"minInterval\": 0,\n",
       "            \"splitLine\": {\n",
       "                \"show\": false,\n",
       "                \"lineStyle\": {\n",
       "                    \"width\": 1,\n",
       "                    \"opacity\": 1,\n",
       "                    \"curveness\": 0,\n",
       "                    \"type\": \"solid\"\n",
       "                }\n",
       "            },\n",
       "            \"data\": [\n",
       "                \"\\u886c\\u886b\",\n",
       "                \"\\u6bdb\\u8863\",\n",
       "                \"\\u9886\\u5e26\",\n",
       "                \"\\u88e4\\u5b50\",\n",
       "                \"\\u98ce\\u8863\",\n",
       "                \"\\u9ad8\\u8ddf\\u978b\",\n",
       "                \"\\u889c\\u5b50\"\n",
       "            ]\n",
       "        }\n",
       "    ],\n",
       "    \"yAxis\": [\n",
       "        {\n",
       "            \"show\": true,\n",
       "            \"scale\": false,\n",
       "            \"nameLocation\": \"end\",\n",
       "            \"nameGap\": 15,\n",
       "            \"gridIndex\": 0,\n",
       "            \"inverse\": false,\n",
       "            \"offset\": 0,\n",
       "            \"splitNumber\": 5,\n",
       "            \"minInterval\": 0,\n",
       "            \"splitLine\": {\n",
       "                \"show\": false,\n",
       "                \"lineStyle\": {\n",
       "                    \"width\": 1,\n",
       "                    \"opacity\": 1,\n",
       "                    \"curveness\": 0,\n",
       "                    \"type\": \"solid\"\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"title\": [\n",
       "        {\n",
       "            \"text\": \"\\u67d0\\u5546\\u573a\\u9500\\u552e\\u60c5\\u51b5\"\n",
       "        }\n",
       "    ]\n",
       "};\n",
       "        chart_acaefe24e05a4155a09d02f10ecf539d.setOption(option_acaefe24e05a4155a09d02f10ecf539d);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<pyecharts.render.display.HTML at 0x7f59ded6e438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyecharts.charts import Bar\n",
    "from pyecharts import options as opts\n",
    "\n",
    "bar = (\n",
    "    Bar()\n",
    "    .add_xaxis([\"衬衫\",\"毛衣\",\"领带\",\"裤子\",\"风衣\",\"高跟鞋\",\"袜子\"])\n",
    "    .add_yaxis(\"商家A\",[114,55,27,101,125,27,105])\n",
    "    .add_yaxis(\"商家B\",[57,134,137,129,145,60,49])\n",
    "    .set_global_opts(title_opts=opts.TitleOpts(title=\"某商场销售情况\"))\n",
    ")\n",
    "bar.render_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
